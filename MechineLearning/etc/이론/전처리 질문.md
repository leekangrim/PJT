# 전처리 질문



# 텍스트 전처리



## 토큰화



## 정제, 정규화



## 어간 추출, 표제어 추출



## 불용어



## 정규 표현식



## 정수 인코딩

컴퓨터는 텍스트보다는 숫자를 더 잘 처리 할 수 있습니다. 이를 위해 자연어 처리에서는 텍스트를 숫자로 바꾸는 여러가지 기법들이 있습니다. 그리고 그러한 기법들을 본격적으로 적용시키기 위한 첫 단계로 각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업이 필요할 때가 있습니다.



- 정수 인코딩
  - 단어에 정수를 부여하는 방법 중 하나로 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법이 있습니다.



## 패딩

자연어 처리를 하다보면 각 문장(또는 문서)은 서로 길이가 다를 수 있습니다. 그런데 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있습니다. 다시 말해 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요할 때가 있습니다.





## 원-핫 인코딩

컴퓨터 또는 기계는 문자보다는 숫자를 더 잘 처리 할 수 있습니다. 이를 위해 자연어 처리에서는 문자를 숫자로 바꾸는 여러가지 기법들이 있습니다. 원-핫 인코딩(One-Hot Encoding)은 그 많은 기법 중에서 단어를 표현하는 가장 기본적인 표현 방법

단어 집합은 서로 다른 단어들의 집합입니다.

단어 집합에 있는 단어들을 가지고, 문자를 숫자(더 구체적으로는 벡터)로 바꾸는 원-핫 인코딩

원-핫 인코딩을 위해서 먼저 해야할 일은 단어 집합을 만드는 일입니다. 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓으면 이를 단어 집합이라고 합니다. 그리고 이 단어 집합에 고유한 숫자를 부여하는 정수 인코딩을 진행합니다



- 원-핫 인코딩 (one-hot encoding)
  - 원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식
  - 이렇게 표현된 벡터를 원-핫 벡터(One-Hot vector)
  - 원-핫 인코딩을 두 가지 과정으로 정리해보겠습니다.
    (1) 각 단어에 고유한 인덱스를 부여합니다. (정수 인코딩)
    (2) 표현하고 싶은 단어의 인덱스의 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여합니다.
- 한계
  - 이러한 표현 방식은 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점
    - 벡터의 차원이 계속 늘어난다고도 표현
    - 원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수
  - 원-핫 벡터는 단어의 유사도를 표현하지 못한다는 단점
    - 단어 간 유사성을 알 수 없다는 단점은 검색 시스템 등에서 심각한 문제
  - 이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 크게 두 가지
    - 첫째는 카운트 기반의 벡터화 방법인 LSA, HAL 등
    - 둘째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등



# 언어 모델



## 언어 모델

- 언어 모델
  - 언어 모델을 만드는 방법은 크게는 **통계를 이용한 방법**과 **인공 신경망을 이용한 방법**으로 구분
  - 언어 모델(Language Model, LM)은 언어라는 현상을 모델링하고자 단어 시퀀스(또는 문장)에 확률을 할당(assign)하는 모델
  - 언어 모델은 **단어 시퀀스에 확률을 할당(assign)**하는 일을 하는 모델
  - 언어 모델은 가장 자연스러운 단어 시퀀스를 찾아내는 모델
  - 단어 시퀀스에 확률을 할당하게 하기 위해서 가장 보편적으로 사용되는 방법은 언어 모델이 **이전 단어들이 주어졌을 때 다음 단어를 예측**하도록 하는 것
  - 다른 유형의 언어 모델로는 주어진 양쪽의 단어들로부터 가운데 비어있는 단어를 예측하는 언어 모델이 있습니다. 이는 마치 고등학생 모의고사에서 문장의 가운데 단어를 비워놓고 어떤 단어인지 맞추는 빈칸 추론 문제와 비슷합니다. 이 유형의 언어 모델은 맨 마지막 BERT

- 언어 모델링
  - 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업
  - 언어 모델이 이전 단어들로부터 다음 단어를 예측하는 일



- 단어 시퀀스의 확률 할당
- 주어진 이전 단어들로부터 다음 단어 예측하기
- 언어 모델의 간단한 직관
  - **비행기를 타려고 공항에 갔는데 지각을 하는 바람에 비행기를 [?]**라는 문장이 있습니다. **'비행기를'** 다음에 어떤 단어가 오게 될지 사람은 쉽게 **'놓쳤다'**라고 예상할 수 있습니다. 우리 지식에 기반하여 나올 수 있는 여러 단어들을 후보에 놓고 놓쳤다는 단어가 나올 확률이 가장 높다고 판단하였기 때문입니다.
  - 그렇다면 기계에게 위 문장을 주고, **'비행기를'** 다음에 나올 단어를 예측해보라고 한다면 과연 어떻게 최대한 정확히 예측할 수 있을까요? 
  - 기계도 비슷합니다. 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 확률을 예측해보고 가장 높은 확률을 가진 단어를 선택합니다. 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 등장 확률을 추정하고 가장 높은 확률을 가진 단어를 선택합니다.
  - 검색 엔진에서의 언어 모델의 예
    - 검색 엔진이 입력된 단어들의 나열에 대해서 다음 단어를 예측하는 언어 모델을 사용하고 있습니다.



## 통계적 언어 모델

- 조건부 확률
- 문자에 대한 확률
- 카운트 기반의 접근
  - 문장의 확률을 구하기 위해서 다음 단어에 대한 예측 확률을 모두 곱한다는 것은 알았습니다. 그렇다면 SLM은 이전 단어로부터 다음 단어에 대한 확률은 어떻게 구할까요? 정답은 카운트에 기반하여 확률을 계산합니다.
- 카운트 기반 접근의 한계 - 희소 문제
  - 언어 모델은 실생활에서 사용되는 언어의 확률 분포를 근사 모델링 합니다. 실제로 정확하게 알아볼 방법은 없겠지만 현실에서도 An adorable little boy가 나왔을 때 is가 나올 확률이라는 것이 존재합니다. 이를 실제 자연어의 확률 분포, 현실에서의 확률 분포라고 명칭합시다. 기계에게 많은 코퍼스를 훈련시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표입니다. 그런데 카운트 기반으로 접근하려고 한다면 갖고있는 코퍼스(corpus). 즉, 다시 말해 기계가 훈련하는 데이터는 정말 방대한 양이 필요합니다.
  - 예를 들어 위와 같이 P(is|An adorable little boy)P(is|An adorable little boy)를 구하는 경우에서 기계가 훈련한 코퍼스에 An adorable little boy is라는 단어 시퀀스가 없었다면 이 단어 시퀀스에 대한 확률은 0이 됩니다. 또는 An adorable little boy라는 단어 시퀀스가 없었다면 분모가 0이 되어 확률은 정의되지 않습니다. 그렇다면 코퍼스에 단어 시퀀스가 없다고 해서 이 확률을 0 또는 정의되지 않는 확률이라고 하는 것이 정확한 모델링 방법일까요? 아닙니다. 현실에선 An adorable little boy is 라는 단어 시퀀스가 존재하고 또 문법에도 적합하므로 정답일 가능성 또한 높습니다. 이와 같이 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 **희소 문제(sparsity problem)**라고 합니다.
  - 위 문제를 완화하는 방법으로 다음 챕터에서 배우는 n-gram이나 이 책에서 다루지는 않지만 스무딩이나 백오프와 같은 여러가지 일반화(generalization) 기법이 존재합니다. 하지만 희소 문제에 대한 근본적인 해결책은 되지 못하였습니다. 결국 이러한 한계로 인해 언어 모델의 트렌드는 통계적 언어 모델에서 인공 신경망 언어 모델로 넘어가게 됩니다.



## N-gram 언어 모델

- 코퍼스에서 카운트하지 못하는 경우의 감소
- N-gram
- N-gram 언어 모델의 한계
- 적용 분야에 맞는 코퍼스의 수집
- 인공신경망을 이용한 언어 모델



## 펄플렉서티(Perplexity)

- 언어모델의 평가 방법 : PPL
- 분기 계수
- 기존 언어 모델 vs 인공 신경망을 이용한 언어 모델



# 카운터 기반 단어 표현



## 다양한 단어 표현 방법

- 단어 표현 방법
- 단어 표현의 카테고리화



## Bag of Word(BoW)



## 문서 단어 행렬 (Document-Term Matrix, DTM)

- DTM 표기법
- DTM 한계



## TF-IDF





# 백터 유사도



## 코사인 유사도



## 여러가지 유사도 기법



## 레벤슈타인 거리를 이용한 오타 정정





# 토픽 모델링



## 잠재 의미 분석 (Latent Semantic Analysis, LSA)



## 잠재 디리클레 할당 (Latent Dirlchlet Allocation, LDA)

