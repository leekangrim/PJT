# BERT 정리

- 자연어 처리 
  - 언어를 컴퓨터가 해독하고 그 의미를 이해하는 기술



- 전처리
  - 용도에 맞게 텍스트를 사전에 처리하는 작업
  - 제대로 된 전처리를 하지 않으면, 자연어 처리 기법들이 제대로 동작하지 않음



- 형태소 분석, 문서 분류, 개체명 인식 등, 대부분 자연어 처리 문제는 **분류**의 문제
  - 의미 분석기
  - 구문 분석기
  - 감성 분석기
  - 형태소 분석기
  - 개채명 인식기
- 자연어 처리의 핵심은 어떻게 분류해낼 것인가?



- 분류 단계
  - 분류를 위해 데이터를 **수학적**으로 표현
  - 분류 대상의 **특징**을 파악 (Feature extraction)
  - 분류 대상이 특징을 기준으로, 분류 대상을 **그래프** 위에 표현 가능
  - 분류 대상들의 경계를 수학적으로 나눌 수 있음 (**Classification**)
  - 새로운 데이터 역시 특징을 기준으로 그래프 표현하면, 어떤 그룹과 유사한지 파악 가능





- 실제로 복잡한 문제들에선 분류 대상의 특징을 사람이 파악하기 어려울 수 있음
- 이러한 특징을 컴퓨터가 스스로 찾고, 스스로 분류하는 것이 '기계학습(딥러닝)'의 핵심 기술





- 분류를 위해, 자연어를 수학적으로 표현
  - 자연어를 좌표평면 위에 표현
- 가장 단순한 표현 방법은 one-hot encoding 방식 
  - 단어 백터가 sparse해서 단어가 가진 의미를 백터 공간에 표현 불가능
- Word2vec 알고리즘 : 자연어의 의미를 백터 공간에 임베딩
- 한 단어의 주변 단어를 통해, 그 단어의 의미를 파악
- Word2vec 알고리즘은 주변부의 단어를 예측하는 방식으로 학습 (Skip-gram 방식)
- Word2vec 특징
  - Sparse representation
    - One-hot encoding
    - n개의 단어데 대한 n차원의 백터
    - 단어가 커질수록 무한대 차원의 백터가 생성
    - 주로 신경망의 입력단에 사용
    - 의미 유추 불가능
    - 차원의 저주 : 차원이 무한대로 커지면 정보 추출이 어려워짐
    - One-hot vector의 차원 축소를 통해 특징을 분류하고자 하는 접근도 있음
  - Dense representation
    - Word embedding
    - 한정된 차원으로 표현 가능
    - 의미 관계 유추 가능
    - 비지도 학습으로 단어의 의미 학습 가능
- Word2vec
  - 단어가 가진 의미 자체를 다차원 공간에 백터화하는 것
  - 중심 단어의 주변 단어들을 이용해 중심단어를 추론하는 방식으로 학습
- Word2vec 장점
  - 단어간의 유사도 측정에 용이
  - 단어간의 관계 파악에 용이
  - 백터 연산을 통해 추론이 가능
- Word2vec 단점
  - 단어의 subword information 무시 (e.g. 서울 vs 서울시 vs 고양시)
  - Out of vocabulary(OOV)에서 적용 불가능

- 한국어는 다양한 용언 형태를 가짐
- Word2vec의 경우, 다양한 용언 표현들이 서로 독립된 vocab으로 관리
- 단어 임베딩을 할때, 용언이나 서브워드들을 활용하면서 임베딩을 하게 되면 더 좋은 임베딩이 되지 않을까?



- Fasttext 훈련
  - 기존의 word2vec과 유사하나, 단어를 n-gram으로 나누어 학습을 수행
  - 이때 n-gram으로 나눠진 단어는 사전에 들어가지 않으며, 별도의 n-gram vector를 형성함
- Fasttext 테스트
  - 입력 단어가 vocabulary에 있는 경우, word2vec와 마찬가리조 해당 단어의 word vector를 return
  - 만약 OOV일 경우, 입력 단어의 n-gram vector들의 합산을 return
    - Fasttext는 단어를 n-gram으로 분리를 한 후, 모든 n-gram vector를 합산한 후 평균을 통해 단어 벡터를 획득
- 오탈자, OVV, 등장 획수가 적은 학습 단어에 대해서 강세
  - 오탈자 입력에 대해서도 본래 단어와 유사한 n-gram이 많아, 유사한 단어 벡터를 획득 가능



- 워드 임베딩은 다른 자연어처리 모델의 입력으로 사용
  - 인공지능 분류를 위해, feature를 입력해야하는데, 이 feature를 워드임베딩을 이용해서, feature로써 사용



- 워드임베딩의 한계점
  - Word2Vec, FastText와 같은 워드임베딩 방식은 동형어, 다의어 등에 대해선 성능이 좋지 못하다는 단점
  - 주변 단어를 통해 학습이 이루어지기 때문에, 문맥을 고려할 수 없음



- 언어 모델
  - 워드임베딩이 자연어를 어떻게 수학적으로 표현하느냐 -> 문맥에 대해서 고려할 수 없음
- 모델의 종류
  - 일기예보 모델, 데이터 모델, 비지니스 모델, ...
- 모델의 특징
  - 자연 법칙을 컴퓨터로 모사함으로써 시뮬레이션 가능
  - 이전 state를 기반으로 미래의 state를 예측
  - 즉 미래의 state를 올바르게 예측하는 방식으로 모델 학습이 가능

- 언어 모델
  - 자연어의 법칙을 컴퓨터로 모사한 모델
  - 주어진 단어들로부터 그 다음에 등장한 단어의 확률을 예측하는 방식으로 학습 (이전 state로 미래 state 예측)
  - 다음 단어의 등장할 단어를 잘 예측하는 모들은 그 언어의 특성이 잘 반영된 모델이자, 문맥을 잘 계산하는 좋은 언어 모델



- Markov 확률 기반의 언어 모델
  - 마코프 체인 모델
  - 초기 언어 모델은 다음 단어나 문자잉 나올 확률을 통계와 단어의 n-gram을 기반으로 계산
  - 딥러닝 기반의 언어모델은 해당 확률을 최대로 하도록 네트워크를 학습



- RNN 기반의 언어 모델
  - RNN 히든 노드가 방향을 가진 엣지로 연결되, 순환하는 구조를 이루는 인공신경망의 한 종류
  - 이전 state 정보가 다음 state를 예측하는데 사용됨으로써, 시계열 데이터 처리에 특화



- RNN 언어 모델을 이용한 application
  - 마지막 출력은 앞선 단어들의 문맥을 고려해서 만들어진 최중 출력 vector -> context vector
  - 출력된 context vector 기반에 대해 classification layer를 붙이면 문장 분류를 위한 신경망 모델
  - 즉, RNN 네트워크를 사용하면, 앞에 나온 단어들의 문맥을 고려한 output을 얻을 수 있고, output을 얻게 되면, 문장을 context vector로 만들어 놓은 것이므로, 좌표평면 위에 표현이 가능하고, 분류의 문제로 넘어갈 수 있으므로, RNN 네트워크 위에 classification을 할 수 있는 layer를 달아놓으면, 분류가 가능해지므로, 기계독해나 긍부정분류도 가능해진다!



<hr>



- Seq2Seq 모델
- Seq2Seq 한계
  - 입력 문장의 길이가 긴 경우, 처음 나온 token에 대한 정보가 희석
  - 고정된 context vector 사이즈로 인해, 긴 문장에 대한 정보를 함축하기 어려움
  - 모든 token이 영향을 미치니, 중요하지 않는 token도 영향을 줌



- Attention 모티브
  - 인간이 정보처리를 할때, 모든 seqence를 고려하면서 정보처리를 하는 것이 아님
  - 인간의 정보처리와 마찬가지로, 중요한 feature는 더욱 중요하게 고려하는 것이 Attention의 모티브
- Attention 개념
  - 기존 Seq2Seq에서는 RNN의 최종 output인 context vector만을 활용
  - Attention에서는 인코더 RNN 셀의 각각 output 활용
  - Decoder에서는 매 step마다 RNN 셀의 output을 이용해 dynamic하게 context vector를 생성
- Attention 진행과정
  - 기존 RNN 모델에서 시작
  - RNN 셀의 각 output들을 입력으로 하는 FFCL
  - 해당 layer의 output을 각 RNN 셀의 score로 결정
  - 출력된 score에 softmax를 취하여 0~1 사이의 값으로 변환
  - 해당 값을 Attention weight로 결정
  - Attention weight와 hidden state를 곱해서 Context vector 획득
  - Decoder의 hidden state가 attention weight 계산에 영향을 줌
    
    - Decoder 결과가 정답과 많이 다르다 = 좋지 못한 context vector = 좋지 못한 attention weight
    - Query와 입력값들간의 상관관계 조정 필요
    - FFCN에서 score를 조정
    - attention weight도 조정됨
- Attetino 모델
  - 문맥에 따라 동적으로 할당되는 encode의 attention weight으로 인한 dynamic context vector를 획득
  - 기존 Seq2Seq의 encoder, decoder 성능을 비약적으로 향상
- Attention 모델  한계
  - 여전희 RNN이 순차적으로 연산이 이뤄짐에 따라 연산속도가 느림



- Self-Attention 모델 모티브
  - Attention is all you need 논문
    - input 만으로도 encoding과 decoding이 잘되고, 언어를 수학적으로 잘 표현할 수 있는 모델 제시
  - RNN을 encoder와 decoder에서 제거
    
    - Decoder 결과가 정답과 많이 다르다 = 좋지 못한 context vector = 좋지 못한 attention weight
    - Query와 입력값들간의 상관관계 조정 필요
    - FFCN에서 score를 조정
    - attention weight도 조정됨
      =>
    - RNN + attention에 적용된 attention은 decoder가 해석하기에 가장 적접한 weight를 찾고자 노력
      =>
    - attention이 decoder가 아니라, input인 값을 가장 잘 표할 수 있도록 학습하면?
    - 자기자신을 가장 잘 표현할 수 있는 좋은 embedding
      =>
    - input 값을 넣고, 그 input 값에 대한 embedding이 잘나옴 == 언어 모델 개념과 비슷

- Self-Attention 목표

  - 자기 자신을 잘 표현하는 attention을 학습하는 것을 목표

- Self-Attention 단계

  - Input 문장의 token을 3종류의 vector로 표현

  1. 임베딩
  2. Concat
  3. 3가지 weight vector 생성(학습 대상)
     1. Query : 얻고자 하는 context vector
     2. Key : Query와의 상관관계를 나타내고자 하는 대상
     3. Value : Query와 Key의 상관관계 값
  4. Query * Key = score 
  5. softmax(score) = attention weight
  6. attention weight * Value = 각각의 중요도
  7. 각각의 중요도를 모두 더하게 되면, 문장 속에서 단어가 지닌 의미!

- Mult-head Self Attention encoder 모델

  - Query, Key, Value로 구성도니 attention layer를 동시에 여러 개 수행
  - 최종적으로 자기 자신을 표현하는 vector 획득

- Transformer 모델

  - Mult-head Self Attention encoder를 여러 층 쌓아서 encoding을 수행



- BERT 모델

  - multi-head self-attention Encoder가 6층으로 이루어짐
    - multi-head self-attention Encoder
      - 12개 self-attention의 출력들을, 열끼리 이어붙여 만든 행렬을 딥러닝 네트워크층을 통과시켜 입력된 자기 자신을 잘 표현하는 벡터로 만들기 위해 학습시킴

- BERT 수행과정

  - vocab 생성
    - WordPiece tokenizing (Byte Pair Encoding : BPE)
    - 빈도수에 기반해서 단어를 의미 있는 패턴으로 잘라서 토큰화

    1. 모든 단어들을 1글자 단위로 자름
    2. 중복을 제거
    3. Bi-gram pair를 만듦
    4. 빈도수가 가장 높은 pair를 찾음 
    5. 이를 중요한 글자쌍이라고 판단하여 하나로 묶음 
    6. 이러한 과정을  지정된 만큼만 반복
    7. 결과로 나온 글자쌍들의 집합을 vocab으로 저장)

  - 입력

    1. 입력된 하나의 문장의 첫글자로 [CLS]를 붙이고 마지막 글자로 [SEP]를 붙임
    2. 문장 내의 단어의 무작위 15%들 중 80%는 [MASK]로 바꾸고 10%는 다른단어로 바꾸고 10% 그대로 유지
    3. 50% 확률로 원래 입력 문장 뒤에 있던 문장을 붙이거나, 다른 문장을 붙여서 입력
    4. 워드 임베딩 / 세그먼트 임베딩 / 포지션 임베딩을 통한 입력

  - 출력

    - 입력 문장을 예측하는 형태의 출력
      1. [MASK]된 단어도 원래 단어를 예측
      2. 다음 문장인지 아닌지 예측

  - 활용분야

    - 현재 문장 다음 이어지는 다음 문장이 문맥상 올바른지 예측
    - 두 질문이 의미상 유사한지
    - 질의에 대한 답변
    - **리뷰 감성분석**
    - 문법적으로 맞는 문장인지 판단
    - 뉴스 헤드라인과 사람이 페러프라이징한 문장이 의미상으로 같은지 판단

