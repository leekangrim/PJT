# 자연어 처리 흐름



- 자연어 처리 : 자연어를 컴퓨터가 해독하고 그 의미를 이해하는 기술
  - 규칙/지식 기반 접근법
  - 확률/통계 기반 접근법
  - TF-IDF를 이용한 키워드 추출 : 많은 문서 중에서 필요없는 키워드는 버리고, 의미있는 키워드를 얻어내자!
    - TF(Term Frequency) : 단어가 문서에 등장한 개수 (TF가 높을수록, 중요한 단어)
    - DF(Document Frequency) : 해당 단어가 등장한 문서의 개수 (DF가 높을수록, 중요하지 않은 단어)



- 전처리 : 데이터를 가지고 학습을 해야하는데, 데이터가 이쁘지 않을 수 있음, 그러면 학습을 할 수 없음, 처리할 수도 없음, 검사 필요, 학습데이터를 많이 쌓는다고 좋은게 아니라, 양질의 데이터를 쌓아야한다.
  - 개행문자 제거
  - 특수문자 제거
  - 공백 제거
  - 중복 표현 제거
  - 이메일, 링크 제거
  - 제목 제거
  - 불용어(의미 없는 용어) 제거
  - 조사 제거
  - 띄어쓰기, 문장분리 보정
  - 사전 구축
- tokenizing : 문장을 통제로 분석은 어렵고, 문장을 의미 있는 단위로 짜름, 한글은 형태소 단위로 자름, 어절단위는 의미를 가진 최소 단위가 아니므로
  - 자연어를 어떤 단위로 살펴볼 것이낙
  - 어절 토큰화
  - 형태소 토큰화
  - 자소 단위 토큰화
  - n-gram 토큰화
  - WordPiece 토큰화
- Lexical anlysis
  - 어휘 분석
  - 형태소 분석
  - 개체명 인식
  - 상호 참조
- Syntactic analysis
  - 구문 분석
- Semantic analysis
  - 의미 분석



- 다양한 자연어 처리 application : 대부분 자연어 처리 문제는 **분류**의 문제
  - 의미 분석기
  - 구문 분석기
  - 감성 분석기
  - 형태소 분석기
  - 개체명 인식기



- 자연어 처리의 핵심은 어떻게 분류해낼 것인가?

  

- 분류 단계
  
  - 분류를 위해 데이터를 수학적으로 표현
  - 분류 대상의 특징을 파악 (Feature extraction)
  - 분류 대상이 특징을 기준으로, 분류 대상을 그래프 위에 표현 가능
  - 분류 대상들의 경계를 수학적으로 나눌 수 있음 (Classification)
  - 새로운 데이터 역시 특징을 기준으로 그래프 표현하면, 어떤 그룹과 유사한지 파악 가능



- 과거에는 사람이 직접 특징을 파악해서 분류
- 실제로 복잡한 문제들에선 분류 대상의 특징을 사람이 파악하기 어려울 수 있음
- 이러한 특징을 컴퓨터가 스스로 찾고, 스스로 분류하는 것이 '기계학습'의 핵심 기술



<hr>



- 자연어를 좌표평면 위에 표현
- 가장 단순한 표현 방법은 one-hot encoding 방식 
  - 단어 백터가 sparse해서 단어가 가진 **의미**를 백터 공간에 표현 불가능
- Word2vec 알고리즘 : 자연어의 의미를 백터 공간에 임베딩
- 한 단어의 주변 단어를 통해, 그 단어의 의미를 파악
- Word2vec 알고리즘은 주변부의 단어를 예측하는 방식으로 학습 (Skip-gram 방식)
- Word2vec 특징
  - Sparse representation
    - One-hot encoding
    - n개의 단어데 대한 n차원의 백터
    - 단어가 커질수록 무한대 차원의 백터가 생성
    - 주로 신경망의 입력단에 사용
    - 의미 유추 불가능
    - 차원의 저주 : 차원이 무한대로 커지면 정보 추출이 어려워짐
    - One-hot vector의 차원 축소를 통해 특징을 분류하고자 하는 접근도 있음
  - Dense representation
    - Word embedding
    - 하정된 차원으로 표현 가능
    - 의미 관계 유추 가능
    - 비지도 학습으로 단어의 의미 학습 가능
- Word2vec
  - 단어가 가진 의미 자체를 다차원 공간에 백터화하는 것
  - 중심 단어의 주변 단어들을 이용해 중심단어를 추론하는 방식으로 학습
- Word2vec 장점
  - 단어간의 유사도 측정에 용이
  - 단어간의 관계 파악에 용이
  - 백터 연산을 통해 추론이 가능
- Word2vec 단점
  - 단어의 subword information 무시 (e.g. 서울 vs 서울시 vs 고양시)
  - Out of vocabulary(OOV)에서 적용 불가능



- 한국어는 다양한 용언 형태를 가짐
- Word2vec의 경우, 다양한 용언 표현들이 서로 독립된 vocab으로 관리
- 단어 임베딩을 할때, 용언이나 서브워드들을 활용하면서 임베딩을 하게 되면 더 좋은 임베딩이 되지 않을까?



- Fasttext 훈련
  - 기존의 word2vec과 유사하나, 단어를 n-gram으로 나누어 학습을 수행
  - 이때 n-gram으로 나눠진 단어는 사전에 들어가지 않으며, 별도의 n-gram vector를 형성함
- Fasttext 테스트
  - 입력 단어가 vocabulary에 있는 경우, word2vec와 마찬가리조 해당 단어의 word vector를 return
  - 만약 OOV일 경우, 입력 단어의 n-gram vector들의 합산을 return
- Fasttext는 단어를 n-gram으로 분리를 한 후, 모든 n-gram vector를 합산한 후 평균을 통해 단어 벡터를 획득
- 오탈자 입력에 대해서도 본래 단어와 유사한 n-gram이 많아, 유사한 단어 벡터를 획득 가능
- 오탈자, OVV, 등장 획수가 적은 학습 단어에 대해서 강세



- 워드 임베딩은 다른 자연어처리 모델의 입력으로 사용
  - 인공지능 분류를 위해, feature를 입력해야하는데, 이 feature를 워드임베딩을 이용해서, feature로써 사용
- 토픽 키워드



- 워드임베딩의 한계점
  - Word2Vec, FastText와 같은 워드임베딩 방식은 동형어, 다의어 등에 대해선 성능이 좋지 못하다는 단점
  - 주변 단어를 통해 학습이 이루어지기 때문에, 문맥을 고려할 수 없음





- 언어 모델
  - 워드임베딩이 자연어를 어떻게 수학적으로 표현하느냐 -> 문맥에 대해서 고려할 수 없음
- 모델
- 모델의 종류
  - 일기예보 모델, 데이터 모델, 비지니스 모델, ...
- 모델의 특징
  - 자연 법칙을 컴퓨터로 모사함으로써 시뮬레이션 가능
  - 이전 state를 기반으로 미래의 state를 예측
  - 즉 미래의 state를 올바르게 예측하는 방식으로 모델 학습이 가능

- 언어 모델
  - 자연어의 법칙을 컴퓨터로 모사한 모델
  - 주어진 단어들로부터 그 다음에 등장한 단어의 확률을 예측하는 방식으로 학습 (이전 state로 미래 state 예측)
  - 다음 단어의 등장할 단어를 잘 예측하는 모들은 그 언어의 특성이 잘 반영된 모델이자, 문맥을 잘 계산하는 좋은 언어 모델



- Markov 확률 기반의 언어 모델
  - 마코프 체인 모델
  - 초기 언어 모델은 다음 단어나 문자잉 나올 확률을 통계와 단어의 n-gram을 기반으로 계산
  - 딥러닝 기반의 언어모델은 해당 확률을 최대로 하도록 네트워크를 학습
- RNN 기반의 언어 모델
  - RNN 히든 노드가 방향을 가진 엣지로 연결되, 순환하는 구조를 이루는 인공신경망의 한 종류
  - 이전 state 정보가 다음 state를 예측하는데 사용됨으로써, 시계열 데이터 처리에 특화
- RNN 언어 모델을 이용한 application
  - 마지막 출력은 앞선 단어들의 문맥을 고려해서 만들어진 최중 출력 vector -> context vector
  - 출력된 context vector 기반에 대해 classification layer를 붙이면 문장 분류를 위한 신경망 모델
  - 즉, RNN 네트워크를 사용하면, 앞에 나온 단어들의 문맥을 고려한 output을 얻을 수 있고, output을 얻게 되면, 문장을 context vector로 만들어 놓은 것이므로, 좌표평면 위에 표현이 가능하고, 분류의 문제로 넘어갈 수 있으므로, RNN 네트워크 위에 classification을 할 수 있는 layer를 달아놓으면, 분류가 가능해지므로, 기계독해나 긍부정분류도 가능해진다!