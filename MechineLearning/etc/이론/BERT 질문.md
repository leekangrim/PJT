# ML 정리



## 0. Word Embedding - Word2Vec (워드 투 벡터)

- Word embedding : 자연어를 좌표평면 위에 표현 (= 단어를 숫자로 바꾸자)

  1.  One-Hot Encoding(원 핫 인코딩 : 1개만 1이고 나머지는 0이다) : 단어를 정수로 표현(데이터에서 빈도수가 가장 높은 단어에게 낮은 인덱스를 부여)하고, 이를 벡터로 표현하면 자신의 인덱스만 1인 [0, 0, ..., 0, 1, 0, ..., 0] 형태의 희소벡터의 형태가 된다.

     - 문제점 : 벡터의 크기도 크고 단어의 의미 파악이 힘들다. (차원의 저주 : 자원이 무한대로 커지면 정보 추출이 어려워짐)

  2. Word2Vec : 단어를 주변단어를 통해 학습시켜 주변 단어들의 문맥을 통해 단어의 의미를 파악, 간단한 신경망 구조, 밀집벡터 형태로 한정된 차원으로 표현 가능, 벡터 연산 가능

     1. CBOW(Continuous Bag of Words) : 주변 단어들을 입력시켜 중심 단어를 출력시키도록 학습
     2. Skip-gram  : 중심 단어를 입력시켜 주변 단어들을 출력시키도록 학습
     
     - 장점 : 단어간 유사도 측정 용이, 단어간 관계 파악 용이, 벡터 연산을 통한 추론 가능
     - 단점 : subword에 대한 정보 무시(종로경찰서는 알지만, 종로와 경찰서는 모름), OOV(Out of vocabulary) 적용 불가능(학습 못한 단어는 추론 불가)
     
  3.  Fasttext : word2vec 학습 뿐만 아니라 단어를 설정한 범위 사이의 크기로 글자를 잘라서 학습시킨다. (apple 2 > ap, pp, pl, le들이 apple을 대신해서 학습), 이를 통해 모르는 단어가 나와도 subword를 합산해서 추론 가능!

      - 오탈자, OOV, 등장 횟수가 적은 단어들에 대해서 강세!
      - 한국어는 자음 + 모음 + 받침 형태로 나누고 Fasttext를 적용하면 맞춤법에도 강한 모습을 보인다 함



<hr>



## 1. RNN

- 현재의 상태를 결정하기 위해서, 현재 단계의 입력과 이전의 상태도 입력으로 받아서 계산. 이를 통해 다음 입력을 예측하로독 학습하게 된다.
- 이는 RNN 네트워크를 사용하면 앞에 문맥(상태)를 고려한 값을 각 셀들의 출력들을 얻을 수 있음
- 마지막 상태는 이전 상태들을 모두 고려한 생태 나오게 된다. 즉, 문장을 입력으로 넣었을 때 마지막 상태는 문장 전체를 고려한 context 벡터이다.  이것을 벡터로써, 이것을 얻게 되면, 좌표 평면에 표현이 가능하고, 좌표 평면에 표현한 순간 분류의 문제로 넘어갈 수 있음
- RNN 언어 모델을 만들고, 마지막에 분류층을 만들면, 간단한 분류가 가능하다!



## 2. Seq2Seq (시퀀스 투 시퀀스)

- 입력이 문장으로 들어가고 출력이 문장으로 나오는 모델
- RNN 인코더 네트워크를 거치면, context 백터가 나오고, 다시 RNN으로 디코더를 하게 되면, context 백터에 대한 해석 결과가 출력
- 변역에 사용



### 문제점 

- 문장이 길어지면 맨처음에 나온 정도들은 점점 희석됨
- 인코더의 최종 출력인 context 백터는 고정된 백터 크기를 가지므로, 문장의 엄청난 데이터들을 모두 함축하기 어렵고, 이를 다시 의미있게 나누는 디코더 작업도 어려워진다.
- context 백터를 만들 때도 그 앞에 쓸데없는 단어들 마져 중요하게 여겨저서 학습에 사용되는 것은 오히려 안좋은 영향을 줄 수 있음
  (인간이 문서를 볼 때 모든 단어를 집중해서 읽지 않고 주요 키워드들을 확인하면서 읽는 것과 같음)



<hr>



## 3. Attention 모델

- Seq2Seq 모델에서 인코더의 각 셀의 출력들이 다음 상태 입력에만 쓰이고 버려지는데 이를 활용
- 이전 셀의 디코더의 출력(= 현재 셀의 디코더의 입력)과 인코더의 각 셀의 출력들이 attention을 통해 얻은 값이 현재 셀의 디코더의 출력을 예측하기 위해 학습됨
  (즉, attention은 디코더가 해석하기 적합한 가중치를 찾기 위해서 학습됨)



### 문제점

- 다음 상태를 예측하기 위해서는 이전 상태를 기다려야하므로 시간이 오래 걸린다.



<hr>



## 4. Self-Attention 모델

- attention으로 디코더의 출력값이 아니라, 인코더 입력값 자체를 잘 표현할 수 있도록 학습되하여 값을 얻을 수 있다면, 인코더의 입력 자기 자신을 가장 잘 표현할 수 있는 context 벡터를 만들 수 있을 것이다. 



<hr>



## 5. BERT

- multi-head self-attention Encoder 모델 : 12개 self-attention의 출력들을, 열끼리 이어붙여 만든 행렬을 딥러닝 네트워크층을 통과시켜 입력된 자기 자신을 잘 표현하는 벡터로 만들기 위해 학습시킴
- BERT : multi-head self-attention Encoder 6층으로 이루어짐
  - 학습이 오래걸리고, 데이터가 많이 필요
    - 그래서 미리 학습시켜 둔다.



- BERT 의 vocab 생성
  - BERT의 WordPiece tokenizing (Byte Pair Encoding : BPE): 빈도수에 기반해서 단어를 의미 있는 패턴으로 잘라서 토큰화
    1. 모든 단어들을 1글자 단위로 자름
    2. 중복을 제거
    3. Bi-gram pair를 만듦
    4. 빈도수가 가장 높은 pair를 찾음 
    5. 이를 중요한 글자쌍이라고 판단하여 하나로 묶음 
    6. 이러한 과정을  지정된 만큼만 반복
    7. 결과로 나온 글자쌍들의 집합을 vocab으로 저장)



- BERT의 입력
  1. 입력된 하나의 문장의 첫글자로 [CLS]를 붙이고 마지막 글자로 [SEP]를 붙임
  2. 문장 내의 단어의 무작위 15%들 중 80%는 [MASK]로 바꾸고 10%는 다른단어로 바꾸고 10% 그대로 유지
  3. 50% 확률로 원래 입력 문장 뒤에 있던 문장을 붙이거나, 다른 문장을 붙여서 입력
  4. 워드 임베딩 / 세그먼트 임베딩 / 포지션 임베딩을 통한 입력
- BERT의 출력
  - 입력 문장을 예측하는 형태의 출력
    1. [MASK]된 단어도 원래 단어를 예측
    2. 다음 문장인지 아닌지 예측



### 활용 분야

- 현재 문장 다음 이어지는 다음 문장이 문맥상 올바른지 예측
- 두 질문이 의미상 유사한지
- 질의에 대한 답변
- **리뷰 감성분석**
- 문법적으로 맞는 문장인지 판단
- 뉴스 헤드라인과 사람이 페러프라이징한 문장이 의미상으로 같은지 판단