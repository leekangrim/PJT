# 정리



- 인공지능
  - 인간이 가진 지적 능력을 컴퓨터를 통해 구현하는 기술
- 머신러닝 (인공지능의 하위 개념)
  - 컴퓨터가 스스로 학습하여, 인공지능의 성능을 향상시키는 방법
- 딥러닝 (머신러닝을 구현하는 기술)
  - 뉴런과 비슷한 인공신경망 방식으로 정보를 처리



- 머신러닝과 딥러닝의 차이
  - 머신러닝 : 중요한 feature를 사람이 직접 제공해주는 수동적인 학습
  - 딥러닝 : 중요한 feature를 컴퓨터가 자동적으로 선별하여 학습



- 머신러닝 목적
  - 주어진 데이터로부터 규칙성을 찾아, 새로운 데이터에 대해서 발견한 규칙성을 기준으로 정답을 찾음
  - 기존의 프로그래밍 방식으로 접근하기 어려웠던 문제의 해결책



- 지도 학습

  - 레이블(Label)이라는 정답과 함께 학습

- 비지도 학습
  - 레이블이 없이 학습



- 혼동 행렬
  - 정밀도(Precision)
    - 예측값이 양성인 데이터의 전체 개수에 대해서 TP의 비율
    - 정밀도=TP/(TP+FP)
  - 재현율(Recall)
    - 실제값이 양성인 데이터의 전체 개수에 대해서 TP의 비율
    - 재현율=TP/(TP+FN)



- 과적합
  - 훈련 데이터를 과하게 학습한 경우
  - 훈련 데이터에 대해서는 오차가 낮지만, 테스트 데이터에 대해서는 오차가 높아지는 상황이 발생
- 과소적합
  -   테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 경우
  -  훈련 데이터에 대해서도 보통 정확도가 낮음

- 과적합을 맊는 방법
  - 데이터 양 늘리기
  - 모델 복작도 줄이기
  - 가중치 규제 적용
  - 드롭아웃
  - 앙상블
  - 조기 종료



- 퍼셉트론
  - 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘



- 단층 퍼셉트론과 다층 퍼셉트론의 차이
  - 단층 퍼셉트론 : 입력층과 출력층만 존재하여, 선형 영역에 대해서만 분리가 가능
  - 다층 퍼셉트론 : 입력층과 출력층 사이에 은닉층(hidden layer) 존재하여, 비선형 영역에 대해서도 분리가 가능
  - 심층 신경망(DNN) : 은닉층이 2개 이상인 신경망



- 인공신경망 종류
  - 피드 포워드 신경망(FFNN) / 순환 신경망 (RNN)
  - 전결합층(FC, Dense layer)



- 활성화 함수
  - 뉴런에서 출력값을 결정하는 함수
  - 비선형 함수
  - 예시
    - 계단 함수
    - 시그모이드 함수
      - 기울기 소실 문제
    - 하이퍼블릭 탄젠트 함수 (hyperbolic tangent function)
    - 렐루 함수 (ReLU)
      - 기울기 소실 문제 해결
      - 죽은 렐루(dying ReLU)
    - **리키 렐루 (Leaky ReLU)**
      - 죽은 렐루(dying ReLU) 보완
    - 소프트맥스 함수
      - 다중 클래스 분류



- 손실 함수
  - 실제값과 예측값의 차이를 수치화해주는 함수
  - 손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치 W와 편향 b를 찾아가는 것이 딥러닝의 학습
  - 예시
    - 오차 제곱 평균 (Mean Squared Error, MSE)
    - 크로스 엔트로피 (Cross-Entropy)



- 옵티마이저
  - 손실 함수의 값을 줄여나가는 알고리즘
  - 종류
    - 배치 경사 하강법(Batch Gradient Descent)
    - 확률적 경사 하강법(SGD)
    - 미니 배치 경사 하강법(Mini - Batch Gradient Descent)
    - 모멘텀(Momentum)
    - 아다그라드(Adagrad)
    - 알 엠 에스 프롭 (RMSprop)
    - 아담(Adam)



- 에포크

  - 에포크란 인공 신경망에서 전체 데이터에 대해서 순전파와 역전파가 끝난 상태
- 배치 사이즈

  - 배치 크기는 몇 개의 데이터 단위로 매개변수를 업데이트 하는지

- 이터레이션

  - 이터레이션이란 한 번의 에포크를 끝내기 위해서 필요한 배치의 수



- 순전파
  - 입력층에서 출력층 방향으로 예측값의 연산이 진행되는 과정
- 역전파
  - 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트

- 기울기 소실
  - 깊은 인공 신경망을 학습하다보면 역전파 과정에서 입력층으로 갈 수록 기울기(Gradient)가 점차적으로 작아지는 현상
- 기울기 소실 방지법
  - ReLU와 ReLU 변형
  - 그래디언트 클리핑 (Gradient Clipping)
    - 기울기 값을 자르는 것. 임계치만큼 크기를 감소
  - 가중치 초기화
    - 가중치 초기화만 적절히 해줘도 기울기 소실 문제과 같은 문제를 완화
    - 예시
      - 세이비어 초기화(Xavier Initialization)
      - He 초기화
  - 배치 정규화
    - 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화
      - 내부 공변량 변화(internal Covariate Shift)
      - 배치 정규화(batch Normalization)
  - 층 정규화





- 정수 인코딩
  - 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여

- 정수 인코딩 필요 이유
  - 컴퓨터는 텍스트보다는 숫자를 더 잘 처리 
  - 이를 위해 자연어 처리에서는 텍스트를 숫자로 바꿈



- 패딩
  - 자연어 처리를 하다보면 각 문장(또는 문서)은 서로 길이가 다를 수 있음
  - 그런데 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있음
  - 다시 말해, 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요



- 원-핫 인코딩
  - 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터
- 원-핫 인코딩 한계
  - 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다
  - 단어의 유사도를 표현하지 못한다



- 언어 모델
  - 언어라는 현상을 모델링하고자, 단어 시퀀스(또는 문장)에 확률을 할당하는 모델
  - 가장 자연스러운 단어 시퀀스를 찾아내는 모델
  - 이전 단어들이 주어졌을 때, 다음 단어를 예측



- 통계학적 언어 모델
  - 카운트 기반 접근 한계
    - 희소 문제 : 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 => 인공 신경망



- 순환 신경망 (RNN)
  - 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델
  - 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징



- 희소 표현
  - 벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현
  - 단점
    - 단어의 개수가 늘어나면 벡터의 차원이 한없이 커진다는 점
    - 공간적 낭비
    - 단어의 의미를 담지 못한다
- 밀집 표현
  - 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤
  - 실수값
- 워드 임베딩(Word Embedding)
  - 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법



- 워드 투 백터(Word2Vec)
  - 분산 표현
    - **비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다 라는 분포 가설**을 가정
    - 벡터에 **단어의 의미를 여러 차원에 분산**하여 표현
    - 벡터의 차원이 상대적으로 **저차원**으로 줄어듬
    - 저차원에 **단어의 의미를 여러 차원에다가 분산**하여 표현
    - 이런 표현 방법을 사용하면 **단어 간 유사도**를 계산 가능
  - CBOW(Continuous Bag of Word)
    - 변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측
  - Skip-Gram
    -  중간에 있는 단어로 주변 단어들을 예측하는 방법

