{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "excited-nelson",
   "metadata": {},
   "source": [
    "# 준비 사항"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "geological-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-audit",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coupled-youth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'nsmc'...\n",
      "Updating files:   6% (1015/14737)\n",
      "Updating files:   7% (1032/14737)\n",
      "Updating files:   8% (1179/14737)\n",
      "Updating files:   9% (1327/14737)\n",
      "Updating files:  10% (1474/14737)\n",
      "Updating files:  11% (1622/14737)\n",
      "Updating files:  12% (1769/14737)\n",
      "Updating files:  13% (1916/14737)\n",
      "Updating files:  14% (2064/14737)\n",
      "Updating files:  15% (2211/14737)\n",
      "Updating files:  15% (2349/14737)\n",
      "Updating files:  16% (2358/14737)\n",
      "Updating files:  17% (2506/14737)\n",
      "Updating files:  18% (2653/14737)\n",
      "Updating files:  19% (2801/14737)\n",
      "Updating files:  20% (2948/14737)\n",
      "Updating files:  21% (3095/14737)\n",
      "Updating files:  22% (3243/14737)\n",
      "Updating files:  23% (3390/14737)\n",
      "Updating files:  23% (3408/14737)\n",
      "Updating files:  24% (3537/14737)\n",
      "Updating files:  25% (3685/14737)\n",
      "Updating files:  26% (3832/14737)\n",
      "Updating files:  27% (3979/14737)\n",
      "Updating files:  28% (4127/14737)\n",
      "Updating files:  29% (4274/14737)\n",
      "Updating files:  30% (4422/14737)\n",
      "Updating files:  30% (4452/14737)\n",
      "Updating files:  31% (4569/14737)\n",
      "Updating files:  32% (4716/14737)\n",
      "Updating files:  33% (4864/14737)\n",
      "Updating files:  34% (5011/14737)\n",
      "Updating files:  35% (5158/14737)\n",
      "Updating files:  36% (5306/14737)\n",
      "Updating files:  36% (5351/14737)\n",
      "Updating files:  37% (5453/14737)\n",
      "Updating files:  38% (5601/14737)\n",
      "Updating files:  39% (5748/14737)\n",
      "Updating files:  40% (5895/14737)\n",
      "Updating files:  41% (6043/14737)\n",
      "Updating files:  42% (6190/14737)\n",
      "Updating files:  43% (6337/14737)\n",
      "Updating files:  44% (6485/14737)\n",
      "Updating files:  44% (6539/14737)\n",
      "Updating files:  45% (6632/14737)\n",
      "Updating files:  46% (6780/14737)\n",
      "Updating files:  47% (6927/14737)\n",
      "Updating files:  48% (7074/14737)\n",
      "Updating files:  49% (7222/14737)\n",
      "Updating files:  50% (7369/14737)\n",
      "Updating files:  51% (7516/14737)\n",
      "Updating files:  52% (7664/14737)\n",
      "Updating files:  53% (7811/14737)\n",
      "Updating files:  54% (7958/14737)\n",
      "Updating files:  55% (8106/14737)\n",
      "Updating files:  56% (8253/14737)\n",
      "Updating files:  56% (8345/14737)\n",
      "Updating files:  57% (8401/14737)\n",
      "Updating files:  58% (8548/14737)\n",
      "Updating files:  59% (8695/14737)\n",
      "Updating files:  60% (8843/14737)\n",
      "Updating files:  61% (8990/14737)\n",
      "Updating files:  62% (9137/14737)\n",
      "Updating files:  63% (9285/14737)\n",
      "Updating files:  64% (9432/14737)\n",
      "Updating files:  65% (9580/14737)\n",
      "Updating files:  66% (9727/14737)\n",
      "Updating files:  67% (9874/14737)\n",
      "Updating files:  67% (9973/14737)\n",
      "Updating files:  68% (10022/14737)\n",
      "Updating files:  69% (10169/14737)\n",
      "Updating files:  70% (10316/14737)\n",
      "Updating files:  71% (10464/14737)\n",
      "Updating files:  72% (10611/14737)\n",
      "Updating files:  73% (10759/14737)\n",
      "Updating files:  74% (10906/14737)\n",
      "Updating files:  75% (11053/14737)\n",
      "Updating files:  76% (11201/14737)\n",
      "Updating files:  77% (11348/14737)\n",
      "Updating files:  77% (11467/14737)\n",
      "Updating files:  78% (11495/14737)\n",
      "Updating files:  79% (11643/14737)\n",
      "Updating files:  80% (11790/14737)\n",
      "Updating files:  81% (11937/14737)\n",
      "Updating files:  82% (12085/14737)\n",
      "Updating files:  83% (12232/14737)\n",
      "Updating files:  84% (12380/14737)\n",
      "Updating files:  85% (12527/14737)\n",
      "Updating files:  86% (12674/14737)\n",
      "Updating files:  87% (12822/14737)\n",
      "Updating files:  88% (12969/14737)\n",
      "Updating files:  89% (13116/14737)\n",
      "Updating files:  89% (13169/14737)\n",
      "Updating files:  90% (13264/14737)\n",
      "Updating files:  91% (13411/14737)\n",
      "Updating files:  92% (13559/14737)\n",
      "Updating files:  93% (13706/14737)\n",
      "Updating files:  94% (13853/14737)\n",
      "Updating files:  95% (14001/14737)\n",
      "Updating files:  96% (14148/14737)\n",
      "Updating files:  97% (14295/14737)\n",
      "Updating files:  98% (14443/14737)\n",
      "Updating files:  99% (14590/14737)\n",
      "Updating files:  99% (14691/14737)\n",
      "Updating files: 100% (14737/14737)\n",
      "Updating files: 100% (14737/14737), done.\n"
     ]
    }
   ],
   "source": [
    "# 네이버 영화리뷰 감정분석 데이터 다운로드\n",
    "!git clone https://github.com/e9t/nsmc.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civilian-pastor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 3)\n",
      "(50000, 3)\n"
     ]
    }
   ],
   "source": [
    "# 판다스로 훈련셋과 테스트셋 데이터 로드\n",
    "train = pd.read_csv(\"nsmc/ratings_train.txt\", sep='\\t')\n",
    "test = pd.read_csv(\"nsmc/ratings_test.txt\", sep='\\t')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cultural-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_csv('total_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "known-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train, test = train_test_split(total, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "experimental-title",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175253</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>캐러멜 매력 적 후식 제품 형태 유지 관심 나 조금 안 맞다 하나 부록 줄 알다 퓌...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147126</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>연말 연 시 정말 강력 추천</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54857</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>책 특정 요리 어떻다 만들다 맛 식 레시피 보여주다 책 아니다 오히려 요리 대한 근...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185454</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>일러스트 카드 3 개 중 리야 카드 1 개 안 오다 누락 같다 받다 방법 없다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142666</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>번역 체 인하다 간혹 호흡 길어지다 문장 많다 그렇다 중요하다 내용</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288022</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>종교 종교 철학 이면 철학</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373906</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>권하다 쉬다 않다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352322</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>나름 기대하다 영 아니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177191</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>다른 국가 주택 정책 살펴보다 마지막 우리나라 상황 점검 기회 제공 사진 통해 이해...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336618</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>뭐 이영화 질 포르노 수준</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  rating                                             review\n",
       "175253      0       6  캐러멜 매력 적 후식 제품 형태 유지 관심 나 조금 안 맞다 하나 부록 줄 알다 퓌...\n",
       "147126      1      10                                    연말 연 시 정말 강력 추천\n",
       "54857       1      10  책 특정 요리 어떻다 만들다 맛 식 레시피 보여주다 책 아니다 오히려 요리 대한 근...\n",
       "185454      1      10        일러스트 카드 3 개 중 리야 카드 1 개 안 오다 누락 같다 받다 방법 없다\n",
       "142666      1      10              번역 체 인하다 간혹 호흡 길어지다 문장 많다 그렇다 중요하다 내용\n",
       "288022      1       5                                     종교 종교 철학 이면 철학\n",
       "373906      0       5                                          권하다 쉬다 않다\n",
       "352322      0       5                                      나름 기대하다 영 아니다\n",
       "177191      0       6  다른 국가 주택 정책 살펴보다 마지막 우리나라 상황 점검 기회 제공 사진 통해 이해...\n",
       "336618      0       5                                     뭐 이영화 질 포르노 수준"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련셋의 앞부분 출력\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-colorado",
   "metadata": {},
   "source": [
    "# 전처리 - 훈련셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dynamic-commander",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175253    캐러멜 매력 적 후식 제품 형태 유지 관심 나 조금 안 맞다 하나 부록 줄 알다 퓌...\n",
       "147126                                      연말 연 시 정말 강력 추천\n",
       "54857     책 특정 요리 어떻다 만들다 맛 식 레시피 보여주다 책 아니다 오히려 요리 대한 근...\n",
       "185454          일러스트 카드 3 개 중 리야 카드 1 개 안 오다 누락 같다 받다 방법 없다\n",
       "142666                번역 체 인하다 간혹 호흡 길어지다 문장 많다 그렇다 중요하다 내용\n",
       "288022                                       종교 종교 철학 이면 철학\n",
       "373906                                            권하다 쉬다 않다\n",
       "352322                                        나름 기대하다 영 아니다\n",
       "177191    다른 국가 주택 정책 살펴보다 마지막 우리나라 상황 점검 기회 제공 사진 통해 이해...\n",
       "336618                                       뭐 이영화 질 포르노 수준\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰 문장 추출\n",
    "sentences = train['review']\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "entertaining-george",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 캐러멜 매력 적 후식 제품 형태 유지 관심 나 조금 안 맞다 하나 부록 줄 알다 퓌레 첨가 물들다 만들다 방법 시판 용 대체 되어다 없다 많이 아쉽다 캐러멜화 이론 적 부분 많다 도움 되어다 [SEP]',\n",
       " '[CLS] 연말 연 시 정말 강력 추천 [SEP]',\n",
       " '[CLS] 책 특정 요리 어떻다 만들다 맛 식 레시피 보여주다 책 아니다 오히려 요리 대한 근본 적 이면 쉬다 알 없다 중요하다 의문 알 갈다 책 [SEP]',\n",
       " '[CLS] 일러스트 카드 3 개 중 리야 카드 1 개 안 오다 누락 같다 받다 방법 없다 [SEP]',\n",
       " '[CLS] 번역 체 인하다 간혹 호흡 길어지다 문장 많다 그렇다 중요하다 내용 [SEP]',\n",
       " '[CLS] 종교 종교 철학 이면 철학 [SEP]',\n",
       " '[CLS] 권하다 쉬다 않다 [SEP]',\n",
       " '[CLS] 나름 기대하다 영 아니다 [SEP]',\n",
       " '[CLS] 다른 국가 주택 정책 살펴보다 마지막 우리나라 상황 점검 기회 제공 사진 통해 이해 도움 단 주택 미래 예측 어렵다 건 당연하다 일 ㅠ [SEP]',\n",
       " '[CLS] 뭐 이영화 질 포르노 수준 [SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT의 입력 형식에 맞게 변환\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "occasional-crest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라벨 추출\n",
    "labels = train['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "owned-quarter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 캐러멜 매력 적 후식 제품 형태 유지 관심 나 조금 안 맞다 하나 부록 줄 알다 퓌레 첨가 물들다 만들다 방법 시판 용 대체 되어다 없다 많이 아쉽다 캐러멜화 이론 적 부분 많다 도움 되어다 [SEP]\n",
      "['[CLS]', '캐', '##러', '##멜', '매', '##력', '적', '후', '##식', '제', '##품', '형', '##태', '유', '##지', '관', '##심', '나', '조', '##금', '안', '맞', '##다', '하', '##나', '부', '##록', '줄', '알', '##다', '[UNK]', '첨', '##가', '물', '##들', '##다', '만', '##들', '##다', '방', '##법', '시', '##판', '용', '대', '##체', '되어', '##다', '없다', '많이', '아', '##쉽', '##다', '캐', '##러', '##멜', '##화', '이', '##론', '적', '부', '##분', '많다', '도', '##움', '되어', '##다', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print (sentences[0])\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hourly-evans",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 220\n",
      "리뷰의 평균 길이 : 27.60017057284553\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print('리뷰의 최대 길이 :',max(len(l) for l in tokenized_texts))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, tokenized_texts))/len(tokenized_texts))\n",
    "plt.hist([len(s) for s in tokenized_texts], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mounted-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(X_train, max_len):\n",
    "    cnt = 0\n",
    "    for sent in X_train:\n",
    "        if(len(sent) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(X_train))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fourth-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 128 이하인 샘플의 비율: 99.95234886699568\n"
     ]
    }
   ],
   "source": [
    "max_len = 128\n",
    "below_threshold_len(tokenized_texts, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "critical-gravity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   101,   9792,  30873, 118930,   9258,  28143,   9664,  10003,\n",
       "        21155,   9672,  52951,   9983,  83616,   9625,  12508,   8900,\n",
       "        71013,   8982,   9678,  40032,   9521,   9256,  11903,   9952,\n",
       "        16439,   9365,  31398,   9692,   9524,  11903,    100,   9748,\n",
       "        11287,   9299,  27023,  11903,   9248,  27023,  11903,   9328,\n",
       "        33768,   9485,  33323,   9603,   9069,  29683,  37909,  11903,\n",
       "        39218,  47058,   9519, 119072,  11903,   9792,  30873, 118930,\n",
       "        18227,   9638,  42769,   9664,   9365,  37712, 100313,   9087,\n",
       "       119169,  37909,  11903,    102,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 토큰의 최대 시퀀스 길이\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 토큰을 숫자 인덱스로 변환\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "seeing-property",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 마스크 초기화\n",
    "attention_masks = []\n",
    "\n",
    "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "young-mortality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   101,   9317,   9524,  42144,    100, 106154,   9706,  35866,  32679,\n",
      "          9681,  12692,  11903,   9706,  31720,    100,   9246,  10622,   9405,\n",
      "         61250,   9251,   9680,  16439,   9634,    100,   9321,   9665,  42428,\n",
      "          9551,  11903,   9690,    100,   9666,  14423,   9356,  12508,   9251,\n",
      "         11903,   9379, 119274,  11903,    102,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(0)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "tensor([   101,   9491,  12692, 118625,  12945,   8907,  36210,   8908,  58303,\n",
      "         22283,  11127,  11281,   8843,  11903,   8857,  12030,   9664,   9546,\n",
      "        118879,  11903,   8868,  12945,   9489,  14040,   9960,  40958,   9706,\n",
      "        119285,  37909,  11903,   9625,  24974,  32679,    102,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(1)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 훈련셋과 검증셋으로 분리\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels, \n",
    "                                                                                    random_state=2018, \n",
    "                                                                                    test_size=0.1)\n",
    "\n",
    "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=2018, \n",
    "                                                       test_size=0.1)\n",
    "\n",
    "# 데이터를 파이토치의 텐서로 변환\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\t\t\t\t\n",
    "\n",
    "print(train_inputs[0])\n",
    "print(train_labels[0])\n",
    "print(train_masks[0])\n",
    "print(validation_inputs[0])\n",
    "print(validation_labels[0])\n",
    "print(validation_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "secret-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈\n",
    "batch_size = 16\n",
    "\n",
    "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
    "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-patient",
   "metadata": {},
   "source": [
    "# 전처리 - 테스트셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hungry-aurora",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343388           감동 없다 재미 없다 엊그제 영화 노트북 보다 영화 저하 되어다 보이다 끄다\n",
       "224465                                      생각 한번 더 영화 네 좋다\n",
       "179224                                           책 살다 사람 호구\n",
       "241302                                 최고 말 필요없다 진짜 작가 천 재임\n",
       "97700     학창시절 분 강의 들다 당시 꽃 시가 제자 작품 도용 설 돌다 부인과 해명 강의실 ...\n",
       "240114                                 보고 또 보다 스릴 잇다 ㅋ 잼 잇다\n",
       "133201                                                  NaN\n",
       "98898                               경제 적 해자 워런 버핏 투자 기법 알 책\n",
       "389379    몸 파다 좋다 집 살다 가정부 쓰다 여자 성의 노리개 온 국민 분노하다 포인트 말 ...\n",
       "73172                              참신하다 다시 성악설 어떻다 논리 일지 기대\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰 문장 추출\n",
    "sentences = test['review']\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "corporate-camping",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 감동 없다 재미 없다 엊그제 영화 노트북 보다 영화 저하 되어다 보이다 끄다 [SEP]',\n",
       " '[CLS] 생각 한번 더 영화 네 좋다 [SEP]',\n",
       " '[CLS] 책 살다 사람 호구 [SEP]',\n",
       " '[CLS] 최고 말 필요없다 진짜 작가 천 재임 [SEP]',\n",
       " '[CLS] 학창시절 분 강의 들다 당시 꽃 시가 제자 작품 도용 설 돌다 부인과 해명 강의실 분위기 묘 적 후 어떻다 진실 밝혀지다 모르다 시인 국회의원 나가다 폼 잡다 행보 상해 난 분 그렇다 [SEP]',\n",
       " '[CLS] 보고 또 보다 스릴 잇다 ㅋ 잼 잇다 [SEP]',\n",
       " '[CLS] nan [SEP]',\n",
       " '[CLS] 경제 적 해자 워런 버핏 투자 기법 알 책 [SEP]',\n",
       " '[CLS] 몸 파다 좋다 집 살다 가정부 쓰다 여자 성의 노리개 온 국민 분노하다 포인트 말 안되다 주제 오광록 씨 국어 책 읽다 연기 [SEP]',\n",
       " '[CLS] 참신하다 다시 성악설 어떻다 논리 일지 기대 [SEP]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT의 입력 형식에 맞게 변환\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "front-audience",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라벨 추출\n",
    "labels = test['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "junior-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 감동 없다 재미 없다 엊그제 영화 노트북 보다 영화 저하 되어다 보이다 끄다 [SEP]\n",
      "['[CLS]', '감', '##동', '없다', '재', '##미', '없다', '[UNK]', '영화', '노', '##트', '##북', '보다', '영화', '저', '##하', '되어', '##다', '보', '##이다', '끄', '##다', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print (sentences[0])\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "approximate-thong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   101,   8848,  18778,  39218,   9659,  22458,  39218,    100,\n",
       "        42428,   9022,  15184,  82512, 106154,  42428,   9663,  35506,\n",
       "        37909,  11903,   9356,  11925,   8970,  11903,    102,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 토큰의 최대 시퀀스 길이\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 토큰을 숫자 인덱스로 변환\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "infinite-reading",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 마스크 초기화\n",
    "attention_masks = []\n",
    "\n",
    "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "radio-color",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   101,   8848,  18778,  39218,   9659,  22458,  39218,    100,  42428,\n",
      "          9022,  15184,  82512, 106154,  42428,   9663,  35506,  37909,  11903,\n",
      "          9356,  11925,   8970,  11903,    102,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(0)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 파이토치의 텐서로 변환\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "print(test_inputs[0])\n",
    "print(test_labels[0])\n",
    "print(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "revolutionary-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈\n",
    "batch_size = 16\n",
    "\n",
    "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
    "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-string",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "improved-arctic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# GPU 디바이스 이름 구함\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# GPU 디바이스 이름 검사\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "lucky-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1060\n"
     ]
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "julian-painting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류를 위한 BERT 모델 생성\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "designing-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # 학습률\n",
    "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
    "                )\n",
    "\n",
    "# 에폭수\n",
    "epochs = 4\n",
    "\n",
    "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-friendship",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "greenhouse-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산 함수\n",
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "incorrect-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "equivalent-viking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch   100  of  18,534.    Elapsed: 0:01:06.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-ebc1520d53b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# Backward 수행으로 그래디언트 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# 그래디언트 클리핑\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\multicampus\\desktop\\bert_test\\venv\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\multicampus\\desktop\\bert_test\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "# 에폭만큼 반복\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "        \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device).long() for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward 수행                \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device).long() for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():     \n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # 로스 구함\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-depression",
   "metadata": {},
   "source": [
    "# 테스트 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시작 시간 설정\n",
    "t0 = time.time()\n",
    "\n",
    "# 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "# 변수 초기화\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    # 경과 정보 표시\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    # 배치를 GPU에 넣음\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # 배치에서 데이터 추출\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    \n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-begin",
   "metadata": {},
   "source": [
    "# 새로운 문장 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 변환\n",
    "def convert_input_data(sentences):\n",
    "\n",
    "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # 입력 토큰의 최대 시퀀스 길이\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    # 토큰을 숫자 인덱스로 변환\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    \n",
    "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # 어텐션 마스크 초기화\n",
    "    attention_masks = []\n",
    "\n",
    "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # 데이터를 파이토치의 텐서로 변환\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 테스트\n",
    "def test_sentences(sentences):\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 문장을 입력 데이터로 변환\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "\n",
    "    # 데이터를 GPU에 넣음\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "            \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = test_sentences(['연기는 별로지만 재미 하나는 끝내줌!'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = test_sentences(['주연배우가 아깝다. 총체적 난국...'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
